[
{"https://python.langchain.com/docs/additional_resources": ["Get started by downloading and installing the app. Get started by installing the API. Use the following steps to get started: Installation Quickstart Modules Modules Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resourcesTutorialsYouTube videosGallery Tutorials YouTube videos Gallery  API reference  Additional resources"]},
{"https://python.langchain.com/docs/modules/data_connection/document_transformers/": ["LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise. The simplest example is to split a long document into smaller chunks that can fit into your model's context window.", "Text splitters work as following: Split the text up into small, semantically meaningful chunks (often sentences) Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function) Once you reach that size, make that chunk its own piece of text.", " splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character. By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]", "With integrations like doctran we can do things like translate documents from one language to another, extract desired properties and add them to metadata, and convert conversational dialogue into a Q/A format. tify similar documents and filter out redundancies."]},
{"https://python.langchain.com/docs/modules/data_connection/text_embedding/": ["The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.", "The base Embeddings class in LangChain exposes two methods. The former takes as input multiple texts, while the latter takes a single text. To start we'll need to install the OpenAI Python package. Accessing the API requires an API key, which you can get by creating an account.", " rted Discord Twitter Python Python JS/TS Homepage Blog. rted discord Twitter Twitter PythonJS/TS homepage blog. rts discord Twitter Python JS-TS home page blog."]},
{"https://python.langchain.com/docs/modules/data_connection/vectorstores/": ["A vector store takes care of storing embedded data and performing vector searchfor you. This walkthrough showcases basic functionality related to a vector store. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules Use cases Guides Ecosystem Additional resources  API reference.", "This walkthrough uses the FAISS vector database, which makes use of the Facebook AI Similarity Search (FAISS) library. We want to use OpenAIEmbeddings so we have to get the OpenAI API Key. It is also possible to do a search for documents similar to a given embedding vector."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/": ["A retriever is an interface that returns documents given an unstructured query. A retriever does not need to be able to store documents, only to return (or retrieve) it. Vector stores can be used as the backbone of a retriever.", "LangChain uses Chroma as the vectorstore to index and search embeddings. The main type of Retriever that we focus on is a Vectorstore retriever. To walk through this tutorial, we'll first need to install chromadb.", "In this notebook we will primarily focus on (1) We will start by showing the one-liner for doing so, but then break down what is actually going on. Each of the steps has multiple sub steps and potential configurations. First, let's import some common classes we'll use no matter what. Next in the generic setup,let's specify the document loader we want to use.", "A lot of the magic is being hid in this VectorstoreIndexCreator. There are three main steps going on after the documents are loaded. We will split the documents into chunks, then select which embeddings we want to use. We now create the vectorstore to use as the index.", "We think it's important to have a simple way to create indexes. It's also important to understand what's going on under the hood. Get started Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/chains/how_to/": ["LangChain provides async support for Chains by leveraging the asyncio library. All classes inherited from  Chains are inherited from LangChain. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOData connectionChainsHow toAsync APIDifferent call methodsCustom chainDebugging chains.SerializationFoundationalDocumentsPopular additional memory agents.", "This notebook covers how to load chains from LangChainHub. Chains can be initialized with a Memory object, which will persist data across calls to the chain. Only some chains support this type of serialization. We will grow the number of supported chains over time."]},
{"https://python.langchain.com/docs/modules/chains/foundational/": ["An LLMChain is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents. This notebook demonstrates how to create a chain that dynamically selects the next chain to use for a given input.", "This notebook showcases using a generic transformation chain. o take the output from one call and use it as the input to another. o use the output of one call to transform the input of another."]},
{"https://python.langchain.com/docs/modules/chains/document/": ["Documents are the core chains for working with Documents. They are useful for summarizing documents, answering questions over documents, extracting information from documents, and more. The stuff documents chain is the most straightforward of the document chains.", "The map reduce documents chain first applies an LLM chain to each document individually. It then passes all the new documents to a separate combine documents chain to get a single output. The map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer."]},
{"https://python.langchain.com/docs/modules/chains/popular/": ["The Popular APIChain enables using LLMs to interact with APIs to retrieve relevant information. Construct the chain by providing a question relevant to the provided API documentation. This example showcases question answering over an index.", "Walkthrough demonstrates how to incorporate OpenAI function-calling API's in a chain. This example demonstrates the use of the SQLDatabaseChain for answering questions over a database. A summarization chain can be used to summarize multiple documents. he ConversationalRetrievalQA chain builds on RetrievalZAChain to provide a chat history component."]},
{"https://python.langchain.com/docs/modules/chains/additional/": ["Self-critique chain with constitutional AICausal program-aided language (CPAL) chainExtractionFLAREGraph DB QA chainHugeGraph QA ChainKuzuQA ChainNebulaGraphQAChainGraph QAGraphSparqlQA chainHypothetical Document EmbeddingsBash chainSelf-checking chainMath chainHTTP request chainSummarization checker chainLLM Symbolic MathModerationDynamically selecting from multiple prompts.Retrieval QA using Open", "NFLAREGraph DB QA chainHugeGraph QA ChainKuzuQA ChainNebulaGraphQAChainGraph QAGraphSparqlQA chainHypothetical Document EmbeddingsBash chainSelf-checking chainMath chainHTTP request chainSummarization checker chainLLM Symbolic MathModerationDynamically selecting from multiple prompts.Retrieval QA using OpenAI functions.", "Rieval QA using OpenAI functionsOpenAPI chainOpenAPI calls with Open AI functionsProgram-aided language model (PAL) chainQuestion-Answering CitationsDocument QATagging Vector store-augmented text generation Analyze Document Self-critique chain with constitutional AI Causal program- aided language (CPAL)Chain Extraction FLARE Graph DB QA chain HugeGraph QA Chain KuzuQA Chain NebulaGraphQAChain Graph QA GraphSparqlQA chain Hypot", "The ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.", "This notebook is an implementation of Forward-Looking Active REtrieval augmented generation (FLARE) This notebook shows how to use LLMs to provide a natural language interface to a graph database. This notebook goes over how to do question answering over a graph data structure.", "This notebook goes over how to use Hypothetical Document Embeddings (HyDE), as described in this paper. This notebook showcases using LLMs and Python to Solve Algebraic Equations. Using the request library to get HTML results from a URL.", " Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this, you may often want to append a moderation chain to any LLMChains.", "OpenAI functions allows for structuring of response output. This is often useful in question answering when you want to not only get the final answer but also supporting evidence, citations, etc. This notebook shows an example of using an OpenAPI chain to call an endpoint in natural language.", "This notebook walks through how to use LangChain for text generation over a vector index. Under the hood we'll be using our Document chains. The tagging chain uses the OpenAI functions parameter to specify a schema to tag a document with."]},
{"https://python.langchain.com/docs/modules/memory/#": ["LangChain provides helper utilities for managing and manipulating previous chat messages. It also provides easy ways to incorporate these utilities into chains. By default, Chains and Agents are stateless meaning that they treat each incoming query independently.", "Memory is a concept of state around throughout a user's interactions with an language model. Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages. We will walk through the simplest form of memory: \"buffer\" memory.", "ChatMessageHistory is a super lightweight wrapper for most memory modules. It exposes convenience methods for saving Human messages, AI messages, and then fetching them all. We now show how to use this simple concept in a chain.", "There are plenty of different types of memory, check out our examples to see them all. Get started Discord Twitter Python JS/TS Homepage Blog. Back to the page you came from."]},
{"https://python.langchain.com/docs/modules/memory/#": ["LangChain provides helper utilities for managing and manipulating previous chat messages. It also provides easy ways to incorporate these utilities into chains. By default, Chains and Agents are stateless meaning that they treat each incoming query independently.", "Memory is a concept of state around throughout a user's interactions with an language model. Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages. We will walk through the simplest form of memory: \"buffer\" memory.", "ChatMessageHistory is a super lightweight wrapper for most memory modules. It exposes convenience methods for saving Human messages, AI messages, and then fetching them all. We now show how to use this simple concept in a chain.", "There are plenty of different types of memory, check out our examples to see them all. Get started Discord Twitter Python JS/TS Homepage Blog. Back to the page you came from."]},
{"https://python.langchain.com/docs/modules/agents/agent_types/": ["How-to Tools Toolkits Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Agents Agent types Agents use an LLM to determine which actions to take and in.", "An action can either be using a tool and observing its output, or returning a response to the user. This agent uses the ReAct framework to determine which tool to usebased solely on the tool's description. Certain OpenAI models have been explicitly fine-tuned to detect when a function should to be called and respond.", "The prompt is designed to make the agent helpful and conversational. It uses the ReAct framework to decide which tool to use, and uses memory to remember the previous conversation interactions. This agent is equivalent to the original self ask with search paper, where a Google search API was provided.", "BabyAGI and then the \"Plan-and-Solve\" paper. Action agentsZero-shot ReActStructured input ReActOpenAI FunctionsConversationalSelf ask with searchReAct document store."]},
{"https://python.langchain.com/docs/modules/agents/#": ["The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. There are two main types of agents: Action agents and Plan-and-execute agents.", "Plan-and-execute agents are better for complex or long-running tasks. Often the best approach is to combine the dynamism of an action agent with the planning abilities of a plan- and-execute agent.", "Action agents are wrapped in agent executors, which are responsible for calling the agent. At a high-level a plan-and-execute agent: Receives user input Plans the full sequence of steps to take Executes the steps in order, passing the outputs of past ste", "The most typical implementation is to have the planner be a language model, and the executor be an action agent. The llm-math tool uses an LLM, so we need to pass that in. ps as inputs to future steps."]},
{"https://python.langchain.com/docs/modules/agents/tools/": [" Tools are interfaces that an agent can use to interact with the world. Tools can be generic utilities (e.g. search), other chains, or even other agents. Currently, tools can be loaded with the following snippet:"]},
{"https://python.langchain.com/docs/modules/agents/toolkits/": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook shows how to use an agent to compare two documents. This toolkit is used to interact with the Azure Cognitive Services API to achieve some multimodal capabilities. This notebook walks through connecting a LangChain em.", "LangChain is a toolkit that lets you build agents to interact with arbitrary APIs. This notebook goes over how to use the Jira tool. We can connect LangChain to Office365 email and calendar.", "Browser toolkits let your agent navigate the web and interact with dynamically rendered sites. Some tools bundled within the Browser toolkit include: This notebook showcases an agent designed to interact with a Power BI Dataset. The agent is designed to answer more general questions about a dataset, and recover from errors.", "Use vectorstores. ned to retrieve information from one or more vectorstores, either with or without sources. Use vectorstores to retrieve data from one of a number of different sources, either without or with sources."]},
{"https://python.langchain.com/docs/modules/callbacks/#": ["LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks. You can subscribe to these events by using the callbacks argument available throughout the API.", "The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) These are available in the langchain/callbacks module. lt-in handlers that you can use to get started. The most basic handler is the StdOutCallbackHandler, which simply logs all events to stdout.", " verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, eg. LLMChain(verbose=True), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection.", " method Discord Twitter Twitter Python JS/TS Homepage Blog. method Twitter PythonJS/TS. Homepage blog. method Discord. method Slack Twitter Twitter. method Reddit. method Facebook. method slack. method."]},
{"https://python.langchain.com/docs/use_cases/agent_simulations/": ["CAMEL Role-Playing Autonomous Cooperative AgentsGenerative Agents in LangChainSimulated Environment: GymnasiumMulti-Player Dungeons & DragonsMulti-agent authoritarian speaker selection. Multi-agent decentralized speaker selectionMulti-Agent Simulated environment: Petting Zoo. ChatbotsCode UnderstandingExtractionMulti-modalQA and Chat over DocumentsSummarizationAnalyzing structured data.", "Agent simulations involve interacting one of more agents with each other. They generally involve two main components: Long Term Memory Simulation Environment and Petting Zoo. Specific implementations of agent simulations include: Simulated Environment: Gymnasium.", "Gymnasium (formerly OpenAI Gym). CAMEL: an implementation of the CAMEL (Communicative Agents for \u201cMind\u201d Exploration of Large Scale Language Model Society) paper. Two Player D&D: an example of how to use a generic simulator for two agents to implement a variant of the popular Dungeons & Dragons role playing game. Agent Debates with Tools: how to enable Dialogue Agents to use tools to inform their responses.", "This notebook implements a generative agent based on the paper Generative Agents: Interactive Simulacra of Human Behavior by Park, et. al. Simulated Environment: PettingZoo: an example of how to create a agent-environment interaction loop for multiple agents."]},
{"https://python.langchain.com/docs/use_cases/agents/": ["CAMEL Role-Playing Autonomous Cooperative AgentsCustom Agent with PlugIn Retrieval Plug-and-Plai multi_modal_outp - Your Context-Aware AI Sales AssistantWikibase AgentInteracting with APIsAutonomous (long-running) agentsChatbotsCode UnderstandingExtractionMulti-modalQA and Chat over DocumentsSummarizationAnalyzing structured data Agent simulations Agents BabyAGI User GuideBabyAGI with Tools", "Agents combine the decision making ability of a language model with tools in order to create a system that can execute and implement solutions on your behalf. Before reading any more, it is highlyrecommended to read the documentation in the agent module.", "If you have a specific task you want the agent to accomplish, you have to give it access to the right tools. The built-in LangChain agent types are designed to work well in generic situations. But you may be able to improve performance by modifying the agent implementation.", " AI Plugins: an implementation of an agent that is designed to be able to use all AI plugins. Wikibase Agent: an agent designed to interact with WikibASE. Sales GPT: This notebook demonstrates a Context-Aware AI Sales agent. Multi-Modal Output Agent: a multi-modal output agent that can generate text and images."]},
{"https://python.langchain.com/docs/use_cases/apis": ["This page covers all resources available in LangChain for working with APIs. Lots of data and information is stored behind APIs. If you are just getting started, and you have relatively simple apis, you should get started with chains.", " API Chain Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful,which allows you to use them on larger and more complex schemas."]},
{"https://python.langchain.com/docs/use_cases/autonomous_agents/": ["Autonomous (long-running) agents are agents that designed to be more long running. You give them one or multiple long term agents to interact with. Chatbots are designed to help you understand structured data. Get started by downloading the latest version of this guide.", "Autonomous Agents are fairly experimental and based off of other open-source projects. By implementing these open source projects in LangChain primitives we can get the benefits of LangChain - easy switching and experimenting with multiple LLMs.", "Iginal Repo) Discord Twitter Python JS/TS Homepage Blog. iginal repo)  Twitter PythonJS/TS homepage blog.  iginalrepo) iginalRepo)"]},
{"https://python.langchain.com/docs/use_cases/chatbots/": ["Language models are good at producing text, that makes them ideal for creating chatbots. An important concept to know for Chatbots is memory. Most chat based applications rely on remembering what happened in previous interactions.", "ChatGPT Clone: A notebook walking through how to recreate a ChatGPT-like experience with LangChain. Conversation Agent: How to create an agent optimized for conversation. Memory concepts and examples: Explanation of key concepts related to memory along with how-to's."]},
{"https://python.langchain.com/docs/use_cases/code/": ["Use LangChain, GPT and Activeloop's Deep Lake to work with code base. Analysis of Twitter the-algorithm source code with LangChain and GPT4. SummarizationAnalyzing structured data Agent simulations Agents Interacting with APIs Autonomous (long-running) agents Chatbots Code Understanding", "LangChain is a tool designed to parse GitHub code repositories. By leveraging VectorStores, Conversational RetrieverChain, and GPT-4, it can answer questions in the context of an entire GitHub repository or generate new code.", "Index the code base: Clone the target repository, load all files within, chunk the files, and execute the indexing process. Embedding and Code Store: Code snippets are embedded using a code-aware embedding model and stored in a VectorStore. Construct the Retriever: Conversational retrieverChain searches the VectorStore to identify the most relevant code snippets for a given query.", "The LLM (GPT-4) generates comprehensive, context-aware answers based on retrieved code snippets and conversation history. Customize the retriever settings and define any user-defined filters as needed. Ask questions: Define a list of questions to ask about the codebase, and then use the ConversationalRetrievalChain.", "LangChain codebase analysis with Deep Lake: A notebook walking through how to analyze and do question answering over THIS code base. Conversational Retriever Chain Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/use_cases/extraction": ["Most APIs and databases still deal with structured information. In order to better work with those, it can be useful to extract structured information from text. Examples of this include: Extracting a structured row to insert into a database from a sentence.", "Output parsers are responsible for instructing the LLM to respond in a specific format. In this case, the output parsers specify the format of the data you would like to extract from the document. For a deep dive on extraction, we recommend checking out korcerya."]},
{"https://python.langchain.com/docs/use_cases#": ["Use cases include agent simulations, chatbots, QA and Chat over Documents. Summarization involves creating a smaller summary of multiple longer documents. Guides Ecosystem Additional resources  API reference  Use cases Walkthroughs of common use cases.", " a, whether it be csvs, excel sheets, or SQL tables. Discord Twitter Python JS/TS Homepage Blog. PythonJS/TS.com. The PythonJS.com blog is a community-driven, free-to-use, open-source site."]},
{"https://python.langchain.com/docs/use_cases/question_answering/": ["QA and Chat over Documents are text splitting and QA / ChatQuestion answering over a group chat messages using Activeloop's DeepLakeSummarizationAnalyzing structured data Agent simulations Agents Interacting with APIs Autonomous (long-running) agents Chatbots Code Understanding Extraction Multi-modal QA andChat over Documents.", "Chat and Question-Answering (QA) over data are popular LLM use-cases. LangChain supports Chat and QA on various data types. Unstructured data can be loaded from many sources.", "The splitting, storage, retrieval, and output generation stages are wrapped. Split the Document into chunks for embedding and vector storage. Embed and store the splits in a vector database (Chroma) Document Transformers All can ingest loaded Documents and process them.", "Retrieve relevant splits for any question using similarity_search. Vectorstores are commonly used for retrieval. For example, SVMs (see thread here) can also be used. Documents can be  Markdown files or Code (py or js)", "LangChain has integrations with many open source LLMs that can be run locally. Distill the retrieved documents into an answer using an LLM (e.g., gpt-3.5-turbo) with RetrievalQA chain.", "The load_qa_chain is an easy way to pass documents to an LLM using these various approaches. The ConversationalRetrievalChain uses chat in the Memory buffer. The documentation offers a few extensions, such as streaming and source documents.", "Twitter Python JS/TS Homepage Blog. Twitter PythonJS/TS Facebook page. TwitterPythonJS.com. TwitterPyJS.org. Facebook.com/PyPyJS/. TwitterPythonPython.com, Twitter, Facebook, Twitter."]},
{"https://python.langchain.com/docs/use_cases/summarization": ["Summarization involves creating a smaller summary of multiple longer documents. This can be useful for distilling long documents into the core pieces of information. The recommended way to get started using a summarization chain is: Summarization notebook.", "Python JS/TS is a set of modules for working with documents. The modules are designed to work with documents in a variety of ways. Additional related resources include: Modules for Working with Documents, Discord, Twitter, PythonJS/TS, and more."]},
{"https://python.langchain.com/docs/use_cases/tabular": ["This page covers all resources available in LangChain for working with data in this format. Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables. If you have text data stored in a tabular format, you may want to load the data into a Document.", "Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control. Agents are more complex, and involve multiple queries to the LLM to understand what to do.", "Twitter Python JS/TS Homepage Blog. Twitter PythonJS/TS Facebook page. TwitterPythonJS.com. TwitterPyJS.org. Facebook.com/PyPyJS/. TwitterPythonJson.com, TwitterPyJson, TwitterPythonPythonJS, Facebook."]},
{"https://python.langchain.com/docs/guides/deployments/": ["Large Language Models (LLMs) are rapidly expanding. It's crucial for developers to understand how to effectively deploy these models in production environments. LangChain simplifies the implementation of business logic around these services.", "In this scenario, most of the computational burden is handled by the LLM providers. LangChain simplifies the implementation of business logic around these services. Alternatively, developers can use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns.", "Deploying LLM applications comes with its own set of challenges. It's vital to understand the trade-offs and key considerations when evaluating serving frameworks. LangChain integrates with several open-source projects designed to tackle these issues.", "4/7 service availability involves creating and maintaining several sub-systems. Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics.", "Your application may encounter errors such as exceptions in your model inference or business logic code. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas.", " load balancing is a technique to distribute work evenly across multiple computers, servers, or other resources. Think of it as a traffic officer directing cars (requests) to different roads (servers) There are several strategies for load balancing.", "Deploying LLM services can be costly, especially when you're handling a large volume of user interactions. Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs.", "Auto-scaling can significantly impact the cost of running your application. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements. Spot instances offer substantial cost savings, typically priced at about a third of on-demand instances.", "If you send individual requests to the model, the GPU might not be fully utilized. On the other hand, by batching requests together, you're allowing the GPU to work on multiple tasks at once. This not only leads to cost savings but can also improve the overall latency of your LLM service.", "It's crucial to avoid tying yourself to a solution specific to one particular framework. Strive for infrastructure that is not locked into any specific machine learning library. Deploying systems like LangChain demands the ability to piece together different models and connect them via logic.", " CI/CD pipelines can significantly speed up the iteration process. Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Kubernetes YAML files come into play. They allow you to define your infrastructure in code files.", "Self-hosting modelsResource Management and Auto-Scaling Utilizing Spot Instances Independent Scaling Batching requests Ensuring Rapid Iteration Cloud providers Infrastructure as Code (IaC) CI/CD Discord Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/guides/evaluation/": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "This section of documentation covers how we approach and think about evaluation in LangChain. It can be really hard to evaluate LangChain c. We would recommend people building on top of LangChain approach evaluation.", "Large Language Models (the core of most chains/agents) are terrific few-shot and zero shot learners. This means you are almost always able to get started on a particular task without a large dataset of examples. This is in stark contrast to traditional machine learning where you had to first collect a bunch of datapoints.", "LangChainDatasets is a Community space on Hugging Face. We intend this to be a collection of open source datasets for evaluating common chains and agents. In order to contribute a dataset, you simply need to join the community and then you will be able to upload datasets.", "We have created a bunch of examples combining the above two solutions to show how we internally evaluate chains and agents when we are developing. To facilitate that, we've included a template notebook for community members to use to build their own examples. In addition to the examples we've curated, we also highly welcome contributions here.", "Questions and answers can be evaluated using LLMs and other tools. There are also some more generic resources for evaluation. For example, this example shows how to use LLMs to come up with question/answer examp.", "The Problem The Solution The Examples Other Examples Discord Twitter Python JS/TS Homepage Blog. Hugging Face Datasets: Covers an example of loading and using a dataset from Hugging face for evaluation."]},
{"https://python.langchain.com/docs/ecosystem/integrations/": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", "This page covers how to use the AI21 ecosystem within LangChain. Aim makes it super easy to visualize and debug LangChain executions. Aim tracks inputs and outputs of LLMs and tools, as well as actions of agents.", "Alibaba Cloud Opensearch OpenSearch is a one-stop platform to develop intelligent search services. OpenSearch was built based on the large-scale distributed search engine developed by Alibaba. The team of international scientists, engineers, and innovators researches, develops, and deploys transformative AI.", " API Gateway supports containerized and serverless workloads, as well as web applications. Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point.", "Azure Blob Storage is optimized for storing massive amounts of unstructured data. Amazon Simple Storage Service (Amazon S3) is an object storage service. AZLyrics is a large, legal, every day growing collection of lyrics.", "Learn how to use LangChain with models deployed on Baseten. Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API. Bilibili is one of the most beloved long-form video sites in China. Brave Search is a search engine developed by Brave Software.", "Clarifai provides an AI platform with the full AI lifecycle for data exploration, data labeling, model training, evaluation and inference. Chroma is a database for building AI applications with embeddings. Cohere is a Canadian startup that provides natural language processing models. College Confidential gives information on 3,800+ colleges and universities.", "This page covers how to use the C Transformers library within LangChain. Confluence is a wiki collaboration platform that saves and organizes all of the project-related material. Docugami converts business documents into a Document XML Knowledge Graph. Diffbot is a service to read web pages.", "This page covers how to use the ForefrontAI ecosystem within LangChain. DuckDB is an in-process SQL OLAP database management system. Elasticsearch is a distributed, RESTful search and analytics engine. EverNote is intended for archiving and creating notes.", "This tutorial covers how to use the Google Search API within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example. LangChain is a serverless and cost-effective enterprise data warehouse that works across clouds.", "This page covers how to use the Grobid to parse articles for LangChain. Hologres is a unified real-time data warehousing service developed by Alibaba Cloud. iFixit is the largest, open repair community on the web.", "This page includes MediaWiki XML Dumps. MediaWiki Dumps contain the content of a wiki. This page covers how to use the Marqo ecosystem within LangChain. The Modern Treasury simplifies complex payment operations. It is a unified platform to power products and processes that move money.", "This page demonstrates how to use OpenLLM. OpenWeatherMap provides all essential weather data for a specific location. Motherduck is a managed DuckDB-in-the-cloud service.", "This page covers how to use PromptLayer within LangChain. Psychic is a platform for integrating with SaaS tools like Notion, Zendesk. Ray Serve is a scalable model serving library for building online inference APIs.", "This page covers how to use the Runhouse ecosystem within LangChain. Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows. Shale Protocol provides production-ready inference APIs for open LLMs.", "Spreedly is a service that allows you to securely store credit cards and use them to transact against any number of payment gateways. Stripe is an Irish-American financial services and software as a service (SaaS) company. StarRocks is a High-Performance Analytical Database.", "This page covers how to use the Tair ecosystem within LangChain. Telegram Messenger is a globally accessible freemium, cross-platform, encrypted, cloud-based and centralized instant messaging service. 2markdown service transforms website content into structured markdown files. Twitter is an online social media and social networking service.", "This notebook goes over how to track your LangChain experiments into one centralized Weights and Biases dashboard. To learn more about prompt engineering and the callback please refer to this Report which explains both alongside the resultant dashboards you can expect to see.", "This page covers how to use Yeager.ai to generate LangChain tools and agents. Wikipedia is the largest and most-read reference work in history. WolframAlpha is an answer engine developed by Wolfram Research. YouTube is an online video sharing and social media platform by Google."]},
{"https://python.langchain.com/docs/ecosystem/dependents": ["The Python JS/TS Homepage Blog is now available on Twitter. Use the Twitter feed to help people with reading comprehension and vocabulary. The Homepage blog is also available on Facebook, Twitter, and GitHub."]},
{"https://python.langchain.com/docs/additional_resources/youtube": ["LangChain is an open-source, cloud-based, machine-learning platform. It lets you build, test, and share AI-powered apps with data. It's available in Python, Ruby, and VBScript.", " AI LangChain for LLMs is... basically just an Ansible playbook by David Shapiro. Build your own LLM Apps with LangChain & GPT-Index by 1littlecoder. Learn LangChain in 10min by Sophia Yang 4 Autonomous AI Agents: \u201cWestworld\u201d simulation.", "LangChain is a tool for building language models in Python. Langchain can be used to run language models locally. LangChain can also be used as a tool to talk to PDFs.", "LangChain is an open-source, Python-based, agent-based chat system. It can be used to connect to the Internet and create chatbots. There are tutorials on how to use LangChain and other agents.", "LangChain is a web app that allows users to interact with documents in a secure manner. LangChain is built on top of Skript, a programming language that allows you to write your own programs. Langchain is available in English and German.", "LangChain is an open-source AI app that can be used to create agents and indexes. LangChain is available as a free download from the Google Play Store. Langchain is available in English, Spanish, French, German, and Italian.", "How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide] by Liam Ottley Build a Multilingual PDF Search App with LangChain, Cohere and Bubble by Menlo Park Lab Building a LangChain Agent (code-free!) using Bubble and Flowise. Chat with Multiple Documents Using LangChain by Data Science Basics Llama Index: Chat with Documentation using URL Loader by Merk.", "LangChain is an open source no-code UI visual tool to build applications. LangFlow and Flowise are free and open source tools. LangChain is available in Python and can be downloaded for free.", "LangChain is an open-source chat-based chat system. LangChain can be used to build chatbots. It can also be used as a way to build and analyze custom data. The LangChain website and YouTube channel are updated regularly."]},
{"https://python.langchain.com/docs/get_started/tutorials": ["We could not find what you were looking for. Please contact the owner of the site that linked you to the original URL and let them know their link is broken. Discord Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/modules/model_io/prompts/": ["The new way of programming models is through prompts. A prompt refers to the input to the model. This input is often constructed from multiple components. LangChain provides several classes and functions to make constructing and working with prompts easy."]},
{"https://python.langchain.com/docs/modules/model_io/models/": ["LangChain provides interfaces and integrations for two types of models: LLMs and Chat Models. LLMs are pure text completion models. Chat models take a list of Chat Messages as input and return a Chat Message.", "GPT-4 and Anthropic's Claude are both implemented as Chat Models. Instead of a single string, they take a list of chat messages as input. Usually these messages are labeled with the speaker. And they return a (\"AI\") chat message as output.", "The TS Homepage Blog is a weekly, offbeat look at what's going on in the world of TS. Visit the Homepage blog for more stories, photos, and videos. The Homepage is also on Facebook, Twitter, and Tumblr."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/": ["Modules is a set of tools that lets you create and use structured output parsers. Use these tools to get more structured information than just text back. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules Model I/\u200bO Output parsers Language models output text.", "Output parsers are classes that help structure language model responses. There are two main methods an output parsers must implement: \"Get format instructions\" and \"Parse with prompt\" The prompt is largely provided in the event the OutputParser wants to retry or fix the output."]},
{"https://python.langchain.com/docs/modules/data_connection/": ["LangChain lets you load, transform, store and query your data via Document loaders, Text embedding models, and Vector stores. Many LLM applications require user-specific data that is not part of the model's training set.", "E and search over embedded data Retrievers: Query your data  Discord Twitter Python JS/TS Homepage Blog. e and searchover embedded data e andsearch over embeddedData. e  search overembedded data"]},
{"https://python.langchain.com/docs/modules/chains/": ["LangChain is an API for chaining LLMs. Using an LLM in isolation is fine for simple applications, but more complex applications require chaining. LangChain provides the Chain interface for such \"chained\" applications.", "Chains allow us to combine multiple components together to create a single, coherent application. We can build more complex chains by combining multiple chains together, or by combining chains with other components. For more specifics check out: How-to for walkthroughs of different chain features.", "If there are multiple variables, you can input them all at once using a dictionary. t with it, and then send it to the LLM. You can use a chat model in an LLMChain as well."]},
{"https://python.langchain.com/docs/modules/memory/": ["LangChain provides helper utilities for managing and manipulating previous chat messages. It also provides easy ways to incorporate these utilities into chains. By default, Chains and Agents are stateless meaning that they treat each incoming query independently.", "Memory is a concept of state around throughout a user's interactions with an language model. Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages. We will walk through the simplest form of memory: \"buffer\" memory.", "ChatMessageHistory is a super lightweight wrapper for most memory modules. It exposes convenience methods for saving Human messages, AI messages, and then fetching them all. We now show how to use this simple concept in a chain.", "There are plenty of different types of memory, check out our examples to see them all. Get started Discord Twitter Python JS/TS Homepage Blog. Back to the page you came from."]},
{"https://python.langchain.com/docs/modules/agents/": ["The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. There are two main types of agents: Action agents and Plan-and-execute agents.", "Plan-and-execute agents are better for complex or long-running tasks. Often the best approach is to combine the dynamism of an action agent with the planning abilities of a plan- and-execute agent.", "Action agents are wrapped in agent executors, which are responsible for calling the agent. At a high-level a plan-and-execute agent: Receives user input Plans the full sequence of steps to take Executes the steps in order, passing the outputs of past ste", "The most typical implementation is to have the planner be a language model, and the executor be an action agent. The llm-math tool uses an LLM, so we need to pass that in. ps as inputs to future steps."]},
{"https://python.langchain.com/docs/modules/callbacks/": ["LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks. You can subscribe to these events by using the callbacks argument available throughout the API.", "The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) These are available in the langchain/callbacks module. lt-in handlers that you can use to get started. The most basic handler is the StdOutCallbackHandler, which simply logs all events to stdout.", " verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, eg. LLMChain(verbose=True), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection.", " method Discord Twitter Twitter Python JS/TS Homepage Blog. method Twitter PythonJS/TS. Homepage blog. method Discord. method Slack Twitter Twitter. method Reddit. method Facebook. method slack. method."]},
{"https://python.langchain.com/docs/use_cases": ["Use cases include agent simulations, chatbots, QA and Chat over Documents. Summarization involves creating a smaller summary of multiple longer documents. Guides Ecosystem Additional resources  API reference  Use cases Walkthroughs of common use cases.", " a, whether it be csvs, excel sheets, or SQL tables. Discord Twitter Python JS/TS Homepage Blog. PythonJS/TS.com. The PythonJS.com blog is a community-driven, free-to-use, open-source site."]},
{"https://python.langchain.com/docs/guides": ["The Python language model application is designed to be easy to use in a variety of ways. Use this guide to help you with the development process of your language model app. Use these guides to get started with the language model development process."]},
{"https://python.langchain.com/docs/ecosystem": ["Hwchase17/langchain is a Python JS/TS API. Use this tool to help you get started with your project. Use the following guides to get started: Installation Quickstart ModulesModel I/\u200bOData connectionChainsMemoryAgentsCallbacksModules Model I/\u200bO Data connection Chains Memory Agents Callbacks Modules Use cases Guides EcosystemIntegrationsDependents Integrations Dependents Additional resources  API reference  Ecosystem 153 items Dependents stats for hwch Chase17/ langchain."]},
{"https://python.langchain.com/docs/additional_resources": ["Get started by downloading and installing the app. Get started by installing the API. Use the following steps to get started: Installation Quickstart Modules Modules Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resourcesTutorialsYouTube videosGallery Tutorials YouTube videos Gallery  API reference  Additional resources"]},
{"https://python.langchain.com/docs/get_started/introduction.html": ["We could not find what you were looking for. Please contact the owner of the site that linked you to the original URL and let them know their link is broken. Discord Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/modules/chains/popular/chat_vector_db": ["The ConversationalRetrievalQA chain builds on Retrieval QAChain to provide a chat history component. It first combines the chat history (either explicitly passed in or retrieved from the provided memory)", "ConversationalRetrievalChain is a chain that looks up relevant documents from the retriever. It then passes those documents and the question to a question answering chain to return a response. To create one, you will need a retriever and a vector store.", "LangChain condenses the current question and the chat history into a standalone question. After that, it does retrieval and then answers the question using retrieval augmented generation with a separate model. Here is an example of doing so. You can also easily return source documents from the ConversationalRetrievalChain.", " f combine document chains with the ConversationalRetrievalChain chain. You can also use this chain with the question answering with sources chain. Output from the chain will be streamed to stdout token by token in this example. Get_chat_history function can be used to format the chat_history string."]},
{"https://python.langchain.com/docs/modules/chains/popular/openai_functions": ["This walkthrough demonstrates how to incorporate OpenAI function-calling API's in a chain. We'll go over:  How to use functions to get structured outputs from ChatOpenAI How to create a generic chain that us can use.", "We can take advantage of OpenAI functions to try and force the model to return a particular kind of structured output. We'll use the create_structured_output_chain to create our chain, which takes the desired structured output either as a Pydantic class or as JsonSchema.", "We can pass in functions as Pydantic classes, directly as OpenAI function dicts, or Python functions. To pass Python function in directly, we'll want to make sure our parameters have type hints, we have a docstring, and we use Google Python style docstrings to describe the parameters.", "There are a number of more specific chains that use OpenAI functions. Extraction: very similar to structured output chain, intended for information/entity extraction specifically. Tagging: tag inputs. OpenAPI: take an OpenAPI spec and create + execute valid requests against the API. QA with citations: use Open AI functions ability to extract citations from text."]},
{"https://python.langchain.com/docs/modules/chains/popular/sqlite": ["LangChain is a set of modules that can be used to connect to a variety of databases. LangChain was created to make it easier for developers to build and test applications using the latest version of the Java programming language.", "Follow the instructions to set up your own database using SQLAlchemy. This demonstration uses the Chinook database. For data-sensitive projects, you can specify return_direct=True in the SQLDatabaseChain initialization to directly return the output.", "You can also customize the prompt that is used. If you are querying for several rows of a table you can select the maximum number of results you want to get by using the 'top_k' parameter (default is 10). This is useful for avoiding query results that exceed the prompt max length or consume tokens unnecessarily.", "In some cases, it can be useful to provide custom table information. For example, if you know that the first few rows of a table are uninformative, it could help to manually provide example rows that are more diverse. It is also possible to limit the columns that will be visible to the model if there are unnecessary columns.", "The chain is as follows: Track overrides the sample_rows_in_table_info parameter. This is useful in cases where the number of tables in the database is large. Even this relatively large model will most likely fail to generate more complicated SQL by itself.", "Run the snippet above a few times, or log exceptions in your deployed environment, to collect lots of examples of inputs, table_info and sql_cmd generated by your language model. You could log any executions of your chain that raise exceptions or get direct user feedback in cases where the results are incorrect."]},
{"https://python.langchain.com/docs/modules/chains/popular/summarize": ["A summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a MapReduceDocumentsChain. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOData connectionChainsHow toFoundationalDocumentsPopularAPI chains.", "Use the map_reduce Chain to do summarization. You can also use your own prompts with this chain. For this example we create multiple documents from one long one, but these documents could be fetched in any manner. In this example, we will respond in Italian.", "You can also use prompt with multi input. In this example, we will use a MapReduce chain to answer specific question about our code. This sections shows results of using the refine Chain to do summarization."]},
{"https://python.langchain.com/docs/modules/chains/additional/analyze_document": ["Self-critique chain with constitutional AICausal program-aided language (CPAL) chainExtractionFLAREGraph DB QA chainHugeGraph QA ChainKuzuQA ChainNebulaGraphQAChainGraph QAGraphSparqlQA chainHypothetical Document EmbeddingsBash chainSelf-checking chainMath chainHTTP request chainSummarization checker chainLLM Symbolic MathModerationDynamically selecting from multiple prompts.Retrieval QA using Open", "NFLAREGraph DB QA chainHugeGraph QA ChainKuzuQA ChainNebulaGraphQAChainGraph QAGraphSparqlQA chainHypothetical Document EmbeddingsBash chainSelf-checking chainMath chainHTTP request chainSummarization checker chainLLM Symbolic MathModerationDynamically selecting from multiple prompts.Retrieval QA using OpenAI functions.", "Program-aided language model (PAL) chain Question-Answering Citations Document QA Tagging Vector store-augmented text generation Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Chains Additional Analyze Document The Ana", " lyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain. Let's take a look at this using a question answering chain."]},
{"https://python.langchain.com/docs/modules/agents/how_to/add_memory_openai_functions": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook goes over how to add memory to OpenAI Functions agent. Shared memory across agents and tools Streaming final agent output. Use ToolKits with Open AI Functions Tools Toolkits Callbacks Modules"]},
{"https://python.langchain.com/docs/modules/agents/how_to/agent_vectorstore": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook covers how to combine agents and vectorstores. The recommended method for doing so is to create a RetrievalQA and then use that as a tool in the overall agent. You can do this with multiple different vectordbs, and use the agent as a way to route between them.", " vectorstores are easily usable as tools in agents. It is easy to use answer multi-hop questions that depend on vectorstores using the existing agent framework. If you intend to use the agent as a router and just want to directly return the result of the RetrievalQAChain."]},
{"https://python.langchain.com/docs/modules/agents/how_to/async_agent": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "Async methods are currently supported for the following Tools: GoogleSerperAPIWrapper, SerpAPIwrapper and LLMMathChain. For Tools that have a coroutine implemented, the AgentExecutor will await them directly. Async support for other agent tools are on the roadmap.", "Serial vs. Concurrent Execution Discord Twitter Python JS/TS Homepage Blog. Back to Mail Online home. back to the page you came from. The next step is to find out what the difference is between the two."]},
{"https://python.langchain.com/docs/modules/agents/how_to/chatgpt_clone": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "Use ToolKits with OpenAI Functions to replicate MRKL. Shared memory across agents and tools Streaming final agent output. ng errors Access intermediate steps Cap the max number of iterations Timeouts for agents."]},
{"https://python.langchain.com/docs/modules/agents/how_to/custom-functions-with-openai-functions-agent": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook goes through how to integrate custom functions with OpenAI Functions agent. Use ToolKits with Openai Functions Tools Toolkits Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Agents How-to Custom functions withopenai Functions Agent."]},
{"https://python.langchain.com/docs/modules/agents/how_to/custom_agent": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook goes through how to create your own custom agent. An agent consists of two parts: Shared memory across agents and tools Streaming final agent output Use ToolKits with OpenAI Functions."]},
{"https://python.langchain.com/docs/modules/agents/how_to/custom_agent_with_tool_retrieval": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook builds off of this notebook and assumes familiarity with how agents work. The novel idea introduced is the idea of using retrieval to select the set of tools to use to answer an agent query. This is useful when you have many many tools to select from.", "We will create one legitimate tool (search) and then 99 fake tools. We will use a vectorstore to create embeddings for each tool description. Then, for an incoming query we can do a similarity search for relevant tools. Test this retriever to see if it seems to work.", "Scord is a Python Python JS/TS Homepage Blog. scord is also a Twitter, Facebook, and other social media site. For more information on scord, visit scord.org."]},
{"https://python.langchain.com/docs/modules/agents/how_to/custom_llm_chat_agent": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "An LLM chat agent consists of three parts: Prompt template, ChatModel and Output Parser. This notebook goes through how to create your own custom agent based on a chat model. Use ToolKits with OpenAI Functions to stream final agent output.", "In this notebook we walk through how to create a custom LLM agent. The template should incorporate: tools: which tools the agent has access and how and when to call them. Prompt: This may be necessary to put in the prompt (so that the agent knows to use these tools)", " ediate_steps: These are tuples of previous (AgentAction, Observation) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way. input: generic user input The output parser is responsible for parsing the LLM output into AgentAction and AgentFinish. This usually depends heavily on the prompt used."]},
{"https://python.langchain.com/docs/modules/agents/how_to/custom_mrkl_agent": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook goes through how to create your own custom MRKL agent. The first way to create a custom agent is to use an existing Agent class, but use a custom LLMChain. It is highly recommended that you work with the ZeroShotAgent, as it is the most generalizable one.", "The ZeroShotAgent class is used to create the prompt template. We will use a helper method to ensure that the prompt contains the appropriate instructions. For this exercise, we will give our agent access to Google Search, and we will customize it in that we will have it answer as a pirate.", "There should be a string starting with \"Action:\" and a following string. Both should be separated by a newline. Agents can also work with prompts that require multiple inputs. Custom LLMChain Multiple inputs Discord Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/modules/agents/how_to/custom_multi_action_agenthttps://python.langchain.com/docs/modules/agents/how_to/handle_parsing_errors": ["We could not find what you were looking for. Please contact the owner of the site that linked you to the original URL and let them know their link is broken. Discord Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/modules/agents/how_to/intermediate_steps": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "Use ToolKits with OpenAI Functions. Cap the max number of iterations. Timeouts for agents. Replicating MRKL. Shared memory across agents and tools. Streaming final agent output."]},
{"https://python.langchain.com/docs/modules/agents/how_to/max_iterations": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook walks through how to cap an agent at taking a certain number of steps. This can be useful to ensure that they do not go haywire and take too many steps. For this example, we will use a specifically crafter adversarial example that tries to trick it into continuing.", "Ould specify method generate which then does one FINAL pass through the LLM to generate an output. ould specifying method generate then do one FINAL passed through theLLM to generated anOutput. Discord Twitter Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/modules/agents/how_to/max_time_limit": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook walks through how to cap an agent executor after a certain amount of time. This can be useful for safeguarding against long running agent runs. Replicating MRKL Shared memory across agents and tools Streaming final agent output Use ToolKits with OpenAI Functions.", " method generate which then does one FINAL pass through the LLM to generate an output. Method generate generates an output that is then used to create an image. method generate produces an image that is used to make an image from an image file."]},
{"https://python.langchain.com/docs/modules/agents/how_to/mrkl": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This walkthrough demonstrates how to replicate the MRKL system using agents. Shared memory across agents and tools Streaming final agent output Use ToolKits with OpenAI Functions. With a chat model Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/agents/how_to/use_toolkits_with_openai_functions": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook shows how to use the OpenAI functions agent with arbitrary toolkits. Use ToolKits with OpenAI Functions Tools Toolkits Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Agents"]},
{"https://python.langchain.com/docs/modules/agents/how_to/streaming_stdout_final_only": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "Streaming final agent output can be done with the FinalStreamingStdOut callback. For convenience, the callback automatically strips whitespaces and new line characters when comparing to answer_ prefix_tokens.", "When the parameter stream_ prefix = True is set, the answer prefix itself will also be streamed. This can be useful when the answerPrefix itself is part of the answer. If you don't know the tokenized version of your answer prefix, you can determine it with the following code."]},
{"https://python.langchain.com/docs/modules/agents/how_to/sharedmemory_for_tools": ["How-toAdd Memory to OpenAI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with OpenAI functions AgentCustom agentCustom agent with tool retrievalCustom LLM Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agents.Shared memory across agents and toolsStreaming final agent outputUse ToolKits with Open AI FunctionsToolsToolkitsCallbacks.", "Shared memory across agents and toolsStreaming final agent outputUse ToolKits with OpenAI FunctionsToolsToolkits Agent types How-toAdd Memory to Open AI Functions AgentCombine agents and vector storesAsync APICreate ChatGPT cloneCustom functions with Openai Functions AgentCustom agentCustom agent with tool retrievalCustom LLM AgentCustom LLL Agent (with a ChatModel)Custom MRKL agentCustom multi-action agentHandle parsing errorsAccess intermediate stepsCap the max number of iterationsTimeouts for agentsReplicating MR", "This notebook goes over adding memory to both of an Agent and its tools. The agent has access to a conversation memory, search tool, and a summarization tool. To test the memory of this agent, we can ask a followup question that relies on information in the previous exchange.", "Confirm that the memory was correctly updated. orrectly. For comparison, below is a bad example that uses the same memory for both the Agent and the tool. The final answer is not wrong, but we see the 3rd Human input is actually from the agent."]},
{"https://python.langchain.com/docs/modules/agents/tools/how_to/multi_input_tool": ["Modules are used to help you get started with the API. Use cases Guides Ecosystem Additional resources Additional resources  API reference  Module. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOData connectionChainsMemoryAgentsAgent typesHow-toTools how-toDefining Custom Tools Human-in-the-loop Tool Validation multi- input tools as OpenAI Functions.", "This notebook shows how to use a tool that requires multiple inputs with an agent. The recommended way to do so is with the StructuredTool class. An alternative to the structured tool would be to use the regular Tool class.", "Tter Python JS/TS Homepage Blog. tter PythonJS/TS.com is a blog about Python and its applications. Visit tterPythonJS.com for more information."]},
{"https://python.langchain.com/docs/modules/agents/toolkits/json": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook showcases an agent designed to interact with large JSON/dict objects. The agent is able to iteratively explore the blob to find what it needs to answer the user's question. In the below example, we are using the OpenAPI spec for the OpenAI API.", " API spec. s about the API. Initialization Example: getting the required POST parameters for a request. Discord Twitter Python JS/TS Homepage Blog. Twitter Twitter PythonJS/TS Blog."]},
{"https://python.langchain.com/docs/modules/agents/toolkits/playwright": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "PlayWright Browser Toolkit is used to interact with the browser. While other tools (like the Requests tools) are fine for static sites, Browser toolkits let your agent navigate the web and interact with dynamically rendered sites.", "Several of the browser tools are StructuredTool's, meaning they expect multiple arguments. These aren't compatible (out of the box) with agents older than the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION."]},
{"https://python.langchain.com/docs/modules/agents/toolkits/python": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook showcases an agent designed to write and execute python code to answer a question. This example was created by John Wiseman. Using ZERO_SHOT_REACT_DESCRIPTION agent type. This shows how to initialize the agent using the OPENAI_FUNCTIONS agent type, which is an alternative to the above.", "Using OpenAI Functions Fibonacci Example Training neural net Discord Twitter Python JS/TS Homepage Blog. SHOT_REACT_DESCRIPTION Using OpenAI functions Fibonsacci Example training neural net."]},
{"https://python.langchain.com/docs/modules/agents/toolkits/python": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook showcases an agent designed to write and execute python code to answer a question. This example was created by John Wiseman. Using ZERO_SHOT_REACT_DESCRIPTION agent type. This shows how to initialize the agent using the OPENAI_FUNCTIONS agent type, which is an alternative to the above.", "Using OpenAI Functions Fibonacci Example Training neural net Discord Twitter Python JS/TS Homepage Blog. SHOT_REACT_DESCRIPTION Using OpenAI functions Fibonsacci Example training neural net."]},
{"https://python.langchain.com/docs/modules/agents/toolkits/sql_database": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook showcases an agent designed to interact with a sql databases. The agent builds off of SQLDatabaseChain and is designed to answer more general questions about a database. Note that, as this agent is in active development, all answers might not be correct.", "This uses the example Chinook database. To set it up follow the instructions on. placing the .db file in a notebooks folder at the. root of this repository. This shows how to.initialize the agent using the ZERO_SHOT_REACT_DESCRIPTION agent type."]},
{"https://python.langchain.com/docs/modules/agents/toolkits/vectorstore": ["Pandas Dataframe AgentPlayWright Browser ToolkitPowerBI Dataset AgentPython AgentSpark Dataframe AgentsSpark SQL AgentSQL Database Agent Vectorstore AgentCallbacksModules Model I/\u200bO Data connection Chains Memory AgentsAgent typesHow-toToolsToolkitsAzure Cognitive Services ToolkitCSV AgentDocument ComparisonGmail ToolkitJiraJSON AgentOffice365 ToolkitOpenAPI agentsNatural Language APIs. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook showcases an agent designed to retrieve information from one or more vectorstores. The agent is optimized for routing, so it is a different toolkit and initializer. We can also easily use this to create an agent with multiple vectorstores and use the agent to route between them.", " amples Multiple Vector Stores. Examples Discord Twitter Python JS/TS Homepage Blog. amples. Multiple Vectorstores. Examples. Discord TwitterPythonJS/TS.com. ample Multiple Vector stores. examples."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/async_callbacks": ["Modules Model I/\u200bO Data connection Chains Memory Agents CallbacksHow-to Async callbacksCustom callback handlers Callbacks for custom chains Logging to file Multiple callback handlers Tags Token counting Tracing Integrations Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Callbacks How-to", "If you use a sync CallbackHandler while using an async method to run your llm/chain/tool/agent, it will still work. But it will be called with run_in_executor which can cause issues if your Callback Handler is not thread-safe."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/custom_callbacks": ["You can create a custom handler to set on the object as well. In the example below, we'll implement st in a custom way. Get started by installing the module and following the instructions below.", "Reaming with a custom handler. reams with a Custom handler. Reaming is done by using a custom handle. reaming with the custom handler used in the Python JS/TS code."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/filecallbackhandler": ["Modules are a set of tools that allow you to write your own RESTful web applications. They include: Callbacks, Logging to file, Token counting, Tracing Integrations, and more.", "StdOutCallbackHandler writes the output to file.log. It also uses the loguru library to log other outputs that are not captured by the handler. Now we can open the file output.log to see that the output has been captured."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/token_counting": ["LangChain offers a context manager that allows you to count tokens. Get started by installing LangChain on your Mac or PC. Use LangChain to test your apps and develop your own custom tokens."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/tracing": ["There are two recommended ways to trace your LangChains. Setting the LANGCHAIN_TRACING environment variable to \"true\" is one way to trace a LangChain. There are also two ways to use the LangChain API.", "Using a context manager with tracing_enabled() to trace a particular block of code. Note if the environment variable is set, all code will be traced, regardless of whether or not it's within the context manager."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/tags": ["You can add tags to your callbacks by passing a tags argument to the call()/run()/apply() methods. This is useful for filt. You can also use tags to add custom callbacks to your application.", "You can pass tags to both constructor and request callbacks. These tags are then passed to the tags argument of the \"start\" callback methods. eg. if you want to log all requests made to a specific LLMChain, you can add a tag, and then filter your logs by that tag."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/multiple_callbacks": ["Modules Model I/\u200bO Data connection Chains Memory Agents CallbacksHow-toAsync callbacksCustom callback handlers Callbacks for custom chains Logging to file Multiple callback handlers Tags Token counting Tracing Integrations Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Callbacks How-to Multiple callback handler", "When we pass through CallbackHandlers using the callbacks keyword arg when executing an run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an Agent, it will be used for all callbacks related to the agent."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/filecallbackhandler": ["Modules are a set of tools that allow you to write your own RESTful web applications. They include: Callbacks, Logging to file, Token counting, Tracing Integrations, and more.", "StdOutCallbackHandler writes the output to file.log. It also uses the loguru library to log other outputs that are not captured by the handler. Now we can open the file output.log to see that the output has been captured."]},
{"https://python.langchain.com/docs/modules/callbacks/how_to/custom_chain": ["When you create a custom chain you can easily set it up to use the same callback system as all the other chains. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Callbacks How-to Callbacks for custom chains ", " built-in chains.  now receive a 2nd argument called run_manager which is bound to that run. This is useful when constructing a custom chain. See this guide for more information on how to create custom chains."]},
{"https://python.langchain.com/docs/modules/callbacks/integrations/context": [" context provides product analytics for AI chatbots. It helps you understand how users are interacting with your AI chat products. In this guide we will show you how to integrate with Context. To get your Context API token: Go to the settings page within your Context account.", "The Context callback handler can be used to directly record transcripts between users and AI assistants. It can be also used to record the inputs and outputs of chains. To use the handler, import the handler from Langchain and instantiate it with your Context API token."]},
{"https://python.langchain.com/docs/modules/callbacks/integrations/infino": ["LangChain and Infino can be used to call OpenAI models via LangChain. We now use matplotlib to create graphs of latency, errors and tokens consumed. This example shows how one can track the following: prompt input, response from chatgpt or any other LangChain model.", "Step 5: Stop infino server Discord Twitter Python JS/TS Homepage Blog. stions dataset LangChain OpenAI Q&A. Publish metrics and logs to Infino Create Metric Charts. Full text query on prompt or prompt outputs."]},
{"https://python.langchain.com/docs/ecosystem/integrations/alibabacloud_opensearch": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", "OpenSearch is a one-stop platform to develop intelligent search services. It was built based on the large-scale distributed search engine developed by Alibaba. OpenSearch serves more than 500 business cases in Alibaba Group.", "OpenSearch helps you develop high quality, maintenance-free, and high performance intelligent search services. In specific scenarios, especially test question search and image search scenarios, you can use the vector search feature together with the multimodal search feature to improve the accuracy of search results.", "If you encounter any problems during use, please feel free to contact xingshaomin.xsm@alibaba-inc.com. Purchase an instance and configure it Alibaba Cloud Opensearch Vector Store Wrappers Discord Twitter Python JS/TS."]},
{"https://python.langchain.com/docs/ecosystem/integrations/apify": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", "This page covers how to use Apify within LangChain. Apify is a cloud platform for web scraping and data extraction. This integration enables you run Actors on the Apify platform and load their results into LangChain to feed your vector index.", "Get your Apify API token and either set it as an environment variable (APIFY_API_TOKEN) or pass it to the ApifyWrapper as apify_api_token in the constructor. You can use our ApifyDataset loader to get data from Apify dataset. For a more detailed walkthrough of this loader, see this notebook."]},
{"https://python.langchain.com/docs/ecosystem/integrations/pinecone": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", "This page covers how to use the Pinecone ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific Pinecone wrappers. There exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore, whether for semantic search or example selection.", "Ebook Installation and Setup Vectorstore Discord Twitter Python JS/TS Homepage Blog. ebook Installation and setup Vectorstore discord Twitter PythonJS/TS homepage blog. ebook installation and setup."]},
{"https://python.langchain.com/docs/ecosystem/integrations/sklearn": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", "Scikit-learn is an open source collection of machine learning algorithms. SKLearnVectorStore provides a simple wrapper around the nearest neighbor implementation of the k nearest neighbors. The API can persist the vector store in json, bson (binary json) or Apache Parquet format.", "The SKLearnVectorStore wrapper is available in the Scikit-learn package. It allows you to use it as a vectorstore. For a more detailed walkthrough of the SKLearn VectorStore wrapper, see this notebook."]},
{"https://python.langchain.com/docs/ecosystem/integrations/slack": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", " Slack is an instant messaging program. There isn't any special setup for it. See a usage example for Slack. Slack is a free, open-source messaging service. There are no plans for Slack to become a paid service."]},
{"https://python.langchain.com/docs/ecosystem/integrations/stochasticai": ["WandB Tracing is a free, open-source, web-based tool for building web apps. Use these tools to help you with your next web app. Get started by downloading and installing the latest version of this guide.", "Microsoft OneDrive, PowerPoint, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, and Microsoft Excel are among the tools in this toolkit. The full list of tools can be found at the bottom of the page.", "Facebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators \u2728Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMom", " AI21 Labs Aim Airbyte Airtable Aleph Alpha Alibaba Cloud Opensearch Amazon API Gateway AnalyticDB Annoy Anyscale Apify Argilla Arthur Arxiv AtlasDB AwaDB AWS S3 Directory AZLyrics Azure Blob Storage Azure Cognitive Search Azure OpenAI Banana Baseten Beam Bedrock BiliBili Blackboard Brave Search.", "This page covers how to use the StochasticAI ecosystem within LangChain. It is broken into two parts: installation and setup, and then references to specific StochasticsAI wrappers.", " LLM is a team of software developers based in the U.S. and around the world. LLM uses Python and other programming languages to develop its software. For more information, visit LLM's official website."]},
{"https://python.langchain.com/docs/modules/memory/how_to/adding_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "This notebook goes over how to use the Memory class with an LLMChain. For the purposes of this walkthrough, we will add  the ConversationBufferMemory class. The most important step is setting up the prompt correctly."]},
{"https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "In this notebook, we go over how to add memory to a chain that has multiple inputs. This chain takes as inputs both related documents and a user question. As an example of such a chain, we will addmemory to a question/answering chain."]},
{"https://python.langchain.com/docs/modules/memory/how_to/agent_with_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "This notebook goes over adding memory to an Agent. We are going to create an LLMChain with memory and then use that to create a custom Agent. For the purposes of this exercise, we are Going to Create a simple custom Agent that has access to a search tool and utilizes the ConversationBufferMemory class.", "We can now construct the LLMChain, with the Memory object, and then create the agent. To test the memory of this agent, we can ask a followup question that relies on information in the previous exchange. We can see that the agent remembered that the previous question was about Canada."]},
{"https://python.langchain.com/docs/modules/memory/how_to/agent_with_memory_in_db": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "This notebook goes over adding memory to an Agent where the memory uses an external message store. We are going to create a RedisChatMessageHistory to connect to an external database to store the messages in. We then create an LLMChain using that chat history as memory.", "In this exercise, we are going to create a simple custom Agent that has access to a search tool and utilizes the ConversationBufferMemory class. To test the memory of this agent, we can ask a followup question that relies on information in the previous exchange to be answered correctly. For fun, let's compare this to an agent that does NOT have memory."]},
{"https://python.langchain.com/docs/modules/memory/how_to/buffer": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "This notebook shows how to use ConversationBufferMemory. This memory allows for storing of messages and then extracts the messages. There are plenty of different types of memory, check out our examples to see them all."]},
{"https://python.langchain.com/docs/modules/memory/how_to/buffer_window": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."]},
{"https://python.langchain.com/docs/modules/memory/how_to/conversational_customization": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "This notebook walks through a few ways to customize conversational memory. By default, this is set to \"AI\", but you can set this to be anything you want. The next way to do so is by changing the Human prefix in the conversation summary.", " AI Prefix Human Prefix Discord Twitter Python JS/TS Homepage Blog. flect this naming change. Let's walk through an example of that in the example below. AI prefix Human prefix."]},
{"https://python.langchain.com/docs/modules/memory/how_to/custom_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "In this notebook, we will add a custom memory type to ConversationChain. We will use spacy to extract entities and save information about them in a simple hash table. Then, during the conversation we will look at the input text, extract any entities, and put any info about them into the context.", "Note that this implementation is pretty simple and brittle and probably not useful in a production setting. Its purpose is to showcase that you can add custom memory implementations. For this, we will need spacy. We now define a prompt that takes in information about entities as well as user input. And now we put it all together!"]},
{"https://python.langchain.com/docs/modules/memory/how_to/entity_summary_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "Memory remembers given facts about specific entities in a conversation. It extracts information on entities (using an LLM) and builds up its knowledge about that entity over time. Let's now use it in a chain! We can also inspect the memory store directly."]},
{"https://python.langchain.com/docs/modules/memory/how_to/kg": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "Use the utilities to get history as a list of messages and get knowledge triplets from a new message. This type of memory uses a knowledge graph to recreate memory. Let's now use this in a chain! Using in aChain Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/memory/how_to/summary_buffer": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "ConversationSummaryBufferMemory combines the last two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary. Unlike the previous implementation, it uses token length rather than number of interactions to determine when to flush interactions.", "In a chain. in a chain Discord Twitter Python Python JS/TS Homepage Blog. PythonJS/TS is a Python-based programming language. It's used in the programming languages Python, C, and Ruby."]},
{"https://python.langchain.com/docs/modules/memory/how_to/summary": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.", "We can also get the history as a list of messages (this is useful if you are using this with a chat model) We can also utilize the predict_new_summary method directly. If you have messages outside this class, you can easily initialize the class with ChatMessageHistory. During loading, a summary will be calculated."]},
{"https://python.langchain.com/docs/modules/memory/how_to/kg": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "Use the utilities to get history as a list of messages and get knowledge triplets from a new message. This type of memory uses a knowledge graph to recreate memory. Let's now use this in a chain! Using in aChain Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/memory/how_to/multiple_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", "It is also possible to use multiple memory classes in the same chain. To combine multiple memory Classes, we can use the CombinedMemory class, and then use that. It is possible to combine multiple modules into a single module."]},
{"https://python.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", " VectorStoreRetrieverMemory stores memories in a VectorDB and queries the top-K most \"salient\" docs every time it is called. This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions. Depending on the store you choose, this step may look different.", "n see the prompt. Discord Twitter Python JS/TS Homepage Blog. n see the Prompt. Twitter PythonJS/TS homepage blog. See the prompt for instructions on how to use the command prompt in this application."]},
{"https://python.langchain.com/docs/modules/memory/how_to/vectorstore_retriever_memory": ["Modules are used to add memory to a Multi-Input Chain. Add Message Memory backed by a database to an Agent. Use multiple memory classes in the same chain. Create a custom Memory class to use in multiple chains.", "How to use multiple memory classes in the same chain. How to add memory to a Multi-Input Chain. Adding Message Memory backed by a database to an Agent. on Knowledge Graph Memory How to customize conversational memory.", " VectorStoreRetrieverMemory stores memories in a VectorDB and queries the top-K most \"salient\" docs every time it is called. This differs from most of the other Memory classes in that it doesn't explicitly track the order of interactions. Depending on the store you choose, this step may look different.", "n see the prompt. Discord Twitter Python JS/TS Homepage Blog. n see the Prompt. Twitter PythonJS/TS homepage blog. See the prompt for instructions on how to use the command prompt in this application."]},
{"https://python.langchain.com/docs/guides/evaluation/agent_benchmarking": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "We go over how to benchmark performance of an agent on tasks where it has access to a calculator and a search tool. It is highly reccomended that you do any evaluation/benchmarking with tracing enabled.", "Tracing is a way of looking at a set of datapoints in detail. We can use this to make predictions and evaluate performance. Tracing can also be used to test the performance of an agent."]},
{"https://python.langchain.com/docs/guides/evaluation/agent_vectordb_sota_pg": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "Here we go over how to benchmark performance on a question answering task using an agent to route between multiple vectordatabases. It is highly recommended that you do any evaluation/benchmarking with tracing enabled.", "Tracing is the process of creating indexes over data. We can use this to make predictions and evaluate them. We also use it to filter the data and look at the incorrect examples. We then use a language model to score the predictions programatically.", "Tter Python JS/TS Homepage Blog. tter PythonJS/TS.com is a blog about Python and its applications. Visit tterPythonJS.com for more information."]},
{"https://python.langchain.com/docs/guides/evaluation/benchmarking_template": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "This is an example notebook that can be used to create a benchmarking notebook for a task of your choice. It is highly reccomende and is designed to make it easier for people to experiment.", "This section shows how to set up a chain that can be run on this dataset. Any guide to evaluating performance in a more systematic manner goes here. d that you do any evaluation/benchmarking with tracing enabled."]},
{"https://python.langchain.com/docs/guides/evaluation/comparisons": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "Evaluating an OpenAPI Chain Question Answering Benchmarking: State of the Union Address. Evaluating Agent Trajectories Using Hugging Face Datasets LLM Math. Comparing Chain Outputs Evaluating Custom Criteria.", "Langchain can be used to evaluate LLMs, Chains, or Agents. In this example, you will use gpt-4 to select which output is preferred. If you already have real usage data for your LLM, you can use a representative sample.", "Using the OpenAI Functions Agent, we can calculate confidence intervals. Below, use the Wilson score to estimate the confidence interval. The p-value is used to give a better sense of how confident we are."]},
{"https://python.langchain.com/docs/guides/evaluation/criteria_eval_chain": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "CriteriaEvalChain is a convenient way to predict whether an LLM or Chain's output complies with a set of crite. Evaluating Custom Criteria is a way to test a model's output against a custom rubric or set of criteria.", "CriteriaEvalChain is used to predict whether an output is \"concise\" It can be used to evaluate against a list of default criteria, or against your own custom criteria. The evaluator still predicts whether the output complies with ALL of the criteria provided.", "CriteriaEvalChain can evaluate model outputs against custom criteria, including a custom rubric and constitutional principles. Custom rubrics are similar to principles from Constitutional AI. You can directly use your ConstitutionalPrinciple objects to instantiate the chain."]},
{"https://python.langchain.com/docs/guides/evaluation/data_augmented_question_answering": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "This notebook uses some generic prompts/language models to evaluate an question answering system that uses other sources of data besides what is in the model. For example, this can be used to evaluate a question answer system over you.", "The Critique library allows for simple calculation of various metrics over generated text. We can use the question answering evaluator to evaluate our question answering chain. In addition to predicting whether the answer is correct or incorrect, we can also use other metrics to get a more nuanced view on the quality of the answers.", "Evaluate when the output closely matches with the gold-standard answer. orrect. Evaluate with Other Metrics Discord Twitter Python JS/TS Homepage Blog. setup Examples Evaluate Evaluate"]},
{"https://python.langchain.com/docs/guides/evaluation/generic_agent_evaluation": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "Good evaluation is key for quickly iterating on your agent's prompts and tools. Here we provide an example of how to use the TrajectoryEvalChain to evaluate the efficacy of the actions taken by your agent.", "Evaluating trajectories is a key piece to incorporate alongside tests for agent subcomponents. In this example, you evaluated an agent based its entire \"trajectory\" using the TrajectoryEvalChain. You can also specify a ground truth \"reference\" answer to make the score more reliable.", "Set up and test the Agent. Evaluating the Agent Conclusion Discord Twitter Python JS/TS Homepage Blog.  correctness, etc.) Test the Agent for correctness. Test the agent for testing for correctness, and for evaluation. Tests the Agent to ensure that it is working."]},
{"https://python.langchain.com/docs/guides/evaluation/huggingface_datasets": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "This example shows how to use Hugging Face datasets to evaluate models. For demonstration purposes, we will just evaluate a simple question a state of the Union address. We show how to load examples to evaluation models on from Hugging face's dataset package.", "We load a dataset from Hugging Face, and then convert it to a list of dictionaries. We can now make and inspect the predictions for these questions. Because these answers are more complex than multiple choice, we can now evaluate their accuracy using a language model."]},
{"https://python.langchain.com/docs/guides/evaluation/openapi_eval": ["Agent Benchmarking: Search + Calculator. Data Augmented Question Answering: Using Hugging Face DatasetsLLM MathEvaluating an OpenAPI Chain. Question AnSWering: State of the Union AddressQA GenerationQuestion Answered: ChinookModel Comparison.", "This notebook goes over ways to semantically evaluate an OpenAPI Chain. It calls an endpoint defined by the OpenAPI specification using purely natural language. Load a wrapper of the spec (so we can work with it more easily)", "The API Chain has two main components: Translate the user query to an API request (request synthesizer) and translate the API response to a natural language response. We construct an evaluation chain to grade the request synthesizer against selected human queries. To evaluate a chain against your own endpoint, you'll want to generate a test dataset.", " optional: Generate Input Questions and Request Ground Truth Queries Run the API Chain Evaluate the requests chain. Generating Test Datasets Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/connecting_to_a_feature_store": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "Feature stores are a concept from traditional machine learning that make sure data fed into models is up-to-date and relevant. This concept is extremely relevant when considering putting LLM applicati. Language models Output parsers Data connection Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Model IO Prompts Prompt templates", "In this notebook we will show how to connect prompt templates to feature stores. The basic idea is to call a feature store from inside a prompt template. This assumes you have already run the steps in the README around getting started.", "Use Tecton to create a chain that achieves personalization backed by a feature store. For simplicity, we are only using a single Feature View. More sophisticated applications may require more feature views to retrieve the features needed.", "The above Feature Service is expected to be applied to a live workspace. For this example, we will be using the \"prod\" workspace. Here we will set up a custom TectonPromptTemplate. This prompt template will take in a user_id and look up their stats.", "The input to this prompt template is just avg_transaction, since that is the only user defined piece. All other variables are looked up inside the prompt template. We can now use this in a chain to achieve personalization."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "In this tutorial, we'll learn how to create a prompt template that uses few shot examples. A few shot prompt template can be constructed from either a set of examples, or from an Example Selector object.", "In this tutorial, we will use the SemanticSimilarityExampleSelector class. This class selects few shot examples based on their similarity to the input. We will reuse the example set and the formatter from the previous section.", " create a FewShotPromptTemplate object. This object takes in the example selector and the formatter for the few shot examples. rch.rch is a Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "The LLM can be used to generate English language explanations of a function given its name. To achieve this task, we will create a custom prompt template that takes in the function name as input, and formats the prompt templa.", "LangChain provides a set of default prompt templates that can be used to generate prompts for a variety of tasks. There are essentially two distinct prompt templates available - string prompt templates and chat prompt templates. In this guide, we will create a custom prompt using a string prompt template.", "We will create a custom prompt template that takes in the function name as input and formats the prompt to provide the source code of the function. To achieve this, let's first create a function that will return the sourcecode of a function given its name. Next, we'll create a special prompt template for this function."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples_chat": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "This notebook covers how to use few shot examples in chat models. There does not appear to be solid consensus on how best to do few shot prompting. As a result, we are not solidifying any abstractions around this yet.", "The first way of doing few shot prompting relies on using alternating human/ai messages. OpenAI provides an optional name parameter that they also recommend using in conjunction with system messages. Here is an example of how to do that below."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/format_output": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "The format method is available as string, list of messages and ChatPromptValue. The output of the format method has been changed to use the MessagePrompt template. The format method can also be used to output a list of Message objects."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/formats": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", " Prompt template uses Python f-string as its template format. It can also use other formats like jinja2, specified through the template_format argument. Prompt template by default uses the f- string template, but it can use any other format.", "Only jinja2 and f-string are supported. For other formats, kindly raise an issue on the Github page. Discord Twitter Python JS/TS Homepage Blog. Twitter PythonJS/TS"]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/partial": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "LangChain is an open-source, free-to-use command-line programming tool. LangChain is available in two versions: LangChain 1.0 and LangChain 2.0. It can be used to create and edit a prompt template.", "One common use case for wanting to partial a prompt template is if you get some of the variables before others. The other common use is to partial with a function. We go over the motivations for both use cases as well as how to do it in LangChain.", "A prime example of this is with date or time. You can't hard code it in the prompt, and passing it along with the other input variables is a bit annoying. In this case, it's very handy to be able to partial the prompt with a function that always returns the current date. to fetch in a common way."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/msg_prompt_templates": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "LangChain provides different types of MessagePromptTemplate. The most commonly used are AIMessagePrompt Template, SystemMessagePrompt template and HumanMessagePromPT template. LangChain also provides data connection Chains and Memory Agents.", "LangChain provides ChatMessagePrompt template and MessagesPlaceholder. Chat model supports taking chat message with arbitrary role. Can be useful when you are uncertain of what role you should be using for template."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/prompt_composition": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "This notebook goes over how to compose multiple prompts together. This can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts: Final prompt: This is the final pro.", "Pipeline prompts are a list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/prompt_serialization": ["InstallationQuickstart Introduction Installation Quickstart Modules Modules Model I/\u200bOPromptsPrompt templatesConnecting to a Feature StoreCustom prompt templateFew-shot prompt templatesFew shot examples for chat models.", "This notebook walks through all the different types of prompts in LangChain. It is often preferrable to store prompts not as python code but as files. This can make it easy to share, store, and version prompts.", "Both JSON and YAML are supported. For other assets, like Examples, different serialization methods may be supported. We support specifying everything in one file, or storing different components (templates, examples, etc) in different files and referencing them.", "LangChain supports specifying everything in one file, or storing different components (templates, examples, etc) in different files and referencing them. There is also a single entry point to load prompts from disk, making it easy to load any type of prompt.", "The same would work if you loaded examples from the yaml file. This shows an example of loading a prompt along with an OutputParser from a file. The key changes from example_prompt to example_Prompt_path."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/custom_example_selector": ["Modules Model I/\u200bOPromptsPrompt templatesExample selectorsCustom example selectorSelect by lengthSelect by maximal marginal relevance (MMR)Select by n-gram overlapSelect by similarityLanguage modelsOutput parsersData connectionChainsMemoryAgentsCallbacks", "In this tutorial, we'll create a custom example selector that selects every alternate example from a given list of examples. An ExampleSelector must implement two methods: An add_example method which takes in an example and adds it into the ExampleSelectors. A select_examples method takes in input variables (which are meant to be user input) and returns a list of Examples."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/length_based": ["Modules Model I/\u200bOPromptsPrompt templatesExample selectorsCustom example selectorSelect by lengthSelect by maximal marginal relevance (MMR)Select by n-gram overlapSelect by similarityLanguage modelsOutput parsersData connectionChainsMemoryAgentsCallbacks", "This example selector selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it willselect more."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/mmr": ["Modules Model I/\u200bOPromptsPrompt templatesExample selectorsCustom example selectorSelect by lengthSelect by maximal marginal relevance (MMR)Select by n-gram overlapSelect by similarityLanguage modelsOutput parsersData connectionChainsMemoryAgentsCallbacks", "The MaxMarginalRelevanceExampleSelector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/ngram_overlap": ["Modules Model I/\u200bOPromptsPrompt templatesExample selectorsCustom example selectorSelect by lengthSelect by maximal marginal relevance (MMR)Select by n-gram overlapSelect by similarityLanguage modelsOutput parsersData connectionChainsMemoryAgentsCallbacks", " NGramOverlapExampleSelector selects and orders examples based on which examples are most similar to the input. The ngram overlap score is a float between 0.0 and 1.0, inclusive. The selector allows for a threshold score to be set."]},
{"https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/similarity": ["Modules Model I/\u200bOPromptsPrompt templatesExample selectorsCustom example selectorSelect by lengthSelect by maximal marginal relevance (MMR)Select by n-gram overlapSelect by similarityLanguage modelsOutput parsersData connectionChainsMemoryAgentsCallbacks", "This object selects examples based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity. put parsers Data connection Chains Memory Agents Callbacks Modules"]},
{"https://python.langchain.com/docs/modules/model_io/models/chat/how_to/chat_model_caching": ["LangChain provides an optional caching layer for Chat Models. This is useful for two reasons: \u00a0It provides an\u00a0optional\u00a0caching layer for chat models. It also provides an easy way to test Chat models against different types of data. LangChain is an open-source, free-to-use API.", "It can speed up your application by reducing the number of API calls you make to the LLM provider. It can save you money by reducing your number of calls to LLM providers. If you're often requesting the same completion multiple times, you may need to change the way you use LLM."]},
{"https://python.langchain.com/docs/modules/model_io/models/chat/how_to/human_input_chat_model": ["LangChain provides a pseudo Chat Model class for Caching. Along with HumanInputLLM, LangChain also provides a Pseudo Chat model class. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "This notebook goes over how to use the WikipediaQueryRun tool. It allows you to mock out calls to the Chat Model and simulate how a human would respond if they received the messages. t can be used for testing, debugging, or educational purposes."]},
{"https://python.langchain.com/docs/modules/model_io/models/chat/how_to/llm_chain": ["You can use the existing LLMChain in a very similar way to before - provide a prompt and a mo. You can also use the LLM Chain API in a similar way as before. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "The Python JS/TS Homepage Blog. del.icio.us: http://www.pkjs.com/blog/2013/01/07/ python-js-tss-homepage-blog.html."]},
{"https://python.langchain.com/docs/modules/model_io/models/chat/how_to/prompts": ["Chat models are built around messages, instead of just plain text. You can make us. Use cases Guides Ecosystem Additional resources  API reference  Modules Model I/\u200bO Language models Chat models How-to Prompts", "You can build a ChatPromptTemplate from one or more MessagePromptTemplates. This returns a PromptValue, which you can convert to a string or Message object. For convenience, there is a from_template method exposed on the template. If you were to use this template, this is what it would look like:"]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/async_llm": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "LangChain provides async support for LLMs by leveraging the asyncio library. Currently, OpenAI, PromptLayerOpenAI, ChatOpenAI and Anthropic are supported. You can use the agenerate method to call an OpenAI LLM asynchronously."]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/custom_llm": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "This notebook goes over how to create a custom LLM wrapper. There is only one required thing that a customLLM needs to implement: A _call method that takes in a string, some optional stop words, and returns a string."]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/fake_llm": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "We expose a fake LLM class that can be used for testing. This allows you to mock out calls to the LLM. In this notebook we go over how to use this. We start this with using the FakeLLM in an agent."]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/human_input_llm": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "LangChain provides a pseudo LLM class that can be used for testing, debugging, or educational purposes. This allows you to mock out calls to the LLM and simulate how a human would respond if they received the prompts. In this notebook, we go over how to use this."]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/llm_caching": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "LangChain provides an optional caching layer for LLMs. It can save you money by reducing the number of API calls you make to the LLM provider. You can also turn off caching for particular nodes in chains."]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/streaming_llm": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "Some LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. We support streaming for the OpenAI, ChatOpenAI, and ChatAnthropic implementations."]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/llm_serialization": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "This notebook walks through how to write and read an LLM Configuration to and from disk. This is useful if you want to save the configuration for a given LLM (e.g., the provider, the temperature, etc)"]},
{"https://python.langchain.com/docs/modules/model_io/models/llms/how_to/token_usage_tracking": ["Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart ModulesModel I/\u200bOPromptsLanguage modelsLLMsHow-toAsync APICustom LLMFake LLMHuman input LLMCachingSerializationStreamingTracking token usageIntegrationsChat modelsOutput parsersData connectionChainsMemoryAgentsCallbacksModules Model I/\u200boprompts language models LLMs how-to async API Custom LLM Fake LLM Human input LLM Caching Serialization Streaming Tracking token usage Integrations Chat models Output parsers", "This notebook goes over how to track your token usage for specific calls. It is currently only implemented for the OpenAI API. Anything inside the context manager will get tracked. If a chain or agent with multiple steps in it is used, it will track all those steps."]},
{"https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/csv": ["A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. A record consists of one or more fields, s, s.", "Use the source_column argument to specify a source for the document created from each row. This is useful when using documents loaded from CSV files for chains that answer questions using sources. Load CSV data with a single row per document. See the csv module documentation for more information."]},
{"https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/file_directory": ["Document loaders are used to load all documents in a directory. Under the hood, by default this uses the Unstructured loader. We can use the glob parameter to control which files to loa.", "By default the loading happens in one thread. In order to utilize several threads set the use_multithreading flag to true. By default a progress bar will not be shown. If you need to load Python source code files, use the Python loader.", "We can pass silent_errors to the DirectoryLoader to skip the files which could not be loaded and continue the load process. We can also ask Text loader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class."]},
{"https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/html": ["The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser. This covers how to load HTML documents into a document f. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules", "We can also use BeautifulSoup4 to load HTML documents using the BSHTMLLoader. This will extract the text from the HTML into page_content and the page title as title into metadata. ormat that we can use downstream."]},
{"https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/json": ["Document loaders How-toCSVFile Directory HTML JSON Markdown PDF Integrations Document transformers Text embedding models Vector stores Retrievers Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Data connection Document loaders", "Jq uses a specified jqschema to parse the JSON files. It uses the jq python package. The following demonstrates how metadata can be extracted using the JSON loader. We want to include metadata available in the JSON file into the documents that we create from the content.", "In the current example, we have to tell the loader to iterate over the records in the messages field. This allows us to pass the records (dict) into the metadata_ func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.", "The list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data. The example below shows how we can modify the source to only contain information of the file source relative to the langchain directory."]},
{"https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/markdown": ["How-to Markdown is a lightweight markup language for creating formatted text using a plain-text editor. This covers how to load Markdown documents into a document format that we can use.", "Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\". downstream."]},
{"https://python.langchain.com/docs/modules/data_connection/document_loaders/how_to/pdf": ["How-to PDF Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independe. How-toCSVFile Directory HTML JSON Markdown PDF Integrations Document transformers Text embedding models Vector stores Retrievers Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Data connection Document loaders How- to PDF", "This covers how to load online pdfs into a document format that we can use downstream. Inspired by Daniel Gross's https://gist.com/danielgross/3ab4104e14faccc12b49200843adab21.", "UnstructuredPDFLoader. s a legacy function, and works specifically with Unstructured PDFLoading. This can be helpful for chunking texts semantically into sections. BeautifulSoup can be used to get more structured and rich information about font size, page numbers, pdf headers/footers."]},
{"https://python.langchain.com/docs/modules/data_connection/document_transformers/#": ["LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise. The simplest example is to split a long document into smaller chunks that can fit into your model's context window.", "Text splitters work as following: Split the text up into small, semantically meaningful chunks (often sentences) Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function) Once you reach that size, make that chunk its own piece of text.", " splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character. By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]", "With integrations like doctran we can do things like translate documents from one language to another, extract desired properties and add them to metadata, and convert conversational dialogue into a Q/A format. tify similar documents and filter out redundancies."]},
{"https://python.langchain.com/docs/modules/data_connection/document_transformers/#": ["LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise. The simplest example is to split a long document into smaller chunks that can fit into your model's context window.", "Text splitters work as following: Split the text up into small, semantically meaningful chunks (often sentences) Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function) Once you reach that size, make that chunk its own piece of text.", " splitter takes a list of characters. It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character. By default the characters it tries to split on are [\"\\n\\n\", \"\\n\", \" \", \"\"]", "With integrations like doctran we can do things like translate documents from one language to another, extract desired properties and add them to metadata, and convert conversational dialogue into a Q/A format. tify similar documents and filter out redundancies."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/MultiQueryRetriever": ["Document loaders Document transformers Text embedding models Vector stores Retrievers How-toMultiQueryRetriever Contextual compression Self-querying Time-weighted vector store retriever Vector store-backed retrieverIntegrations ChainsMemoryAgentsCallbacks", "MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents.", "MultiQueryRetriever is a tool to help you get better results from distance-based queries. It uses the LLM to generate queries for you. You can supply a prompt and an output parser to split the results into a list of queries."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/contextual_compression/": ["Document loaders Document transformers Text embedding models Vector stores Retrievers How-toMultiQueryRetriever Contextual compression Self-querying Time-weighted vector store retriever Vector store-backed retrieverIntegrations ChainsMemoryAgentsCallbacks", " contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query. \u201cCompressing\u201d here refers to both compressing the contents of an individual document and filtering out documents wholesale.", "In this. triever, you'll need: a base Retriever a Document Compressor and a ContextualCompressionRetriever. Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks)", "DocumentCompressorPipeline can be used to combine multiple compressors in sequence. The EmbeddingsFilter filter only returns documents that have sufficiently similar embeddings to the query. BaseDocumentTransformers don't perform any contextual compression but simply perform some transformation on a set of documents.", "The Python JS/TS project is a free, open-source, Python-based web app. It lets you test your knowledge of Python and other programming languages. It also provides a way to test Python's ability to answer questions with relevance."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/": ["Chroma self-querying with MyScale, PineconeQdrant, and Retrievers. Time-weighted vector store retriever Vector store-backed retrieverIntegrationsChainsMemoryAgentsCallbacks. Document loaders Document transformers Text embedding models.", "Self-querying retriever is one that, as the name suggests, has the ability to query itself. How-to MultiQueryRetriever Contextual compression Chroma self-queried Self-queries with MyScale Self-Querying with Pinecone Qdrant self-querying Weaviate self- querying Time-weighted vector store retriever Vector store-backed retriever Integrations Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Data connection", "The self-query retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user- input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata.", "We can also use the self query retriever to specify k: the number of documents to fetch. We can do this by passing enable_limit=True to the constructor. t and a short description of the document contents."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/time_weighted_vectorstore": ["Document loaders Document transformers Text embedding models Vector stores Retrievers How-toMultiQueryRetriever Contextual compression Self-querying Time-weighted vector store retriever Vector store-backed retrieverIntegrations ChainsMemoryAgentsCallbacks", "Time-weighted vector store retriever. This retriever uses a combination of semantic similarity and a time decay. A decay rate of 0 means memories never be forgotten. With a high decay rate (e.g., several 9's), the recency score quickly goes to 0!", "The time component is the time component of the Python JS/TS language. The time component can be found on the following social media sites: Discord, Twitter, and PythonJS/TS Homepage."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/": ["Chroma self-querying with MyScale, PineconeQdrant, and Retrievers. Time-weighted vector store retriever Vector store-backed retrieverIntegrationsChainsMemoryAgentsCallbacks. Document loaders Document transformers Text embedding models.", "Self-querying retriever is one that, as the name suggests, has the ability to query itself. How-to MultiQueryRetriever Contextual compression Chroma self-queried Self-queries with MyScale Self-Querying with Pinecone Qdrant self-querying Weaviate self- querying Time-weighted vector store retriever Vector store-backed retriever Integrations Chains Memory Agents Callbacks Modules Use cases Guides Ecosystem Additional resources  API reference  Modules Data connection", "The self-query retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to it's underlying VectorStore. This allows the retriever to not only use the user- input query for semantic similarity comparison with the contents of stored documented, but to also extract filters from the user query on the metadata.", "We can also use the self query retriever to specify k: the number of documents to fetch. We can do this by passing enable_limit=True to the constructor. t and a short description of the document contents."]},
{"https://python.langchain.com/docs/modules/data_connection/retrievers/how_to/vectorstore": ["Document loaders Document transformers Text embedding models Vector stores Retrievers How-toMultiQueryRetriever Contextual compression Self-querying Time-weighted vector store retriever Vector store-backed retrieverIntegrations ChainsMemoryAgentsCallbacks", "A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the Vector Store class to make it conform to the Retriever interface. By default, the vectorstore retriever uses similarity search. You can also a retrieval method that sets a similarity score threshold.", "K to use when doing retrieval. Discord Twitter Twitter Python JS/TS Homepage Blog. ike k to use for retrieving data from the web. ikes k to be used when doing retrieving from the internet."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/comma_separated": ["Modules is a set of tools that allows you to use Python to create your own modules. It includes a List Parser, a Datetime Parser and a Pydantic (JSON) Parser.", "Og. og og  og, og and og. Og, or og eg, i.e. \"How do we get to the top?\""]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/datetime": ["Modules is a set of tools that allows you to use Python JS/TS to create your own modules. Modules uses List parsers, Auto-fixing parsers and Structured output parsers. It also uses Chains Memory Agents and Callbacks."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/enum": ["This notebook shows how to use an Enum output parser. Use cases Guides Ecosystem Additional resources  API reference  Modules Model I/\u200bO Output parsers Enum parsers. Discord Twitter Python JS/TS Homepage Blog."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser": ["Modules is a set of tools that lets you create your own modules. Use cases, use cases, guides, and other resources to help you get started. Get startedIntroductionInstallationQuickstart Introduction Installation Quickstart Modules Modules Use cases Guides Ecosystem Additional resources Additional resources  API reference ", "We can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it. Now we can construct and use a OutputFixingParser. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/pydantic": ["Pydantic (JSON) parsers allows users to specify an arbitrary JSON schema. LLMs can be used to query LLMs for JSON outputs that conform to t. Use cases Guides Ecosystem Additional resources  API reference  Modules Use cases guides Ecosystem.", "Large language models are leaky abstractions. You'll have to use an LLM with sufficient capacity to generate well-formed JSON. In the OpenAI family, DaVinci can do reliably but Curie's ability drops off dramatically. Use Pydantic to declare your data model."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/retry": ["Modules is a set of tools that allows you to write your own Python applications. It's possible to fix any parsing mistakes by only looking at the output, but in some cases it can't. An examp can be used to help you fix mistakes in the output.", "OutputFixing Parsers are used to fix errors in Python. They are used when the output is not just in the incorrect format, but is partially complete. Consider the below example. If we try to parse this response as is, we will get an error. Instead, we can use the RetryOutput Parser to try again."]},
{"https://python.langchain.com/docs/modules/model_io/output_parsers/structured": ["Modules is a set of tools that allows you to build your own web applications. Use these tools to develop your own applications. Get started by downloading and installing the latest version of the Modules software.", "We can now use this to format a prompt to send to the language model, and then parse the returned result. And here's an example of using this in a chat model Discord Twitter Python JS/TS Homepage Blog"]},
{"https://python.langchain.com/docs/modules/memory/integrations/postgres_chat_message_history": ["Modules Model I/\u200bO Data connection Chains MemoryHow-toIntegrations Cassandra Chat Message HistoryDynamodb Chat Message historyEntity Memory with SQLite storage. Mot\u00f6rhead MemoryMot\u00f6rhead memory (Managed) Zep MemoryAgentsCallbacks.", "This notebook goes over how to use Postgres to store chat message history. It also explains how to manage chat messages in Postgres. This notebook also includes a guide to using Postgres in other languages such as Python."]},
{"https://python.langchain.com/docs/modules/memory/integrations/redis_chat_message_history": ["Modules Model I/\u200bO Data connection Chains MemoryHow-toIntegrations Cassandra Chat Message HistoryDynamodb Chat Message historyEntity Memory with SQLite storage. Mot\u00f6rhead MemoryMot\u00f6rhead memory (Managed) Zep MemoryAgentsCallbacks.", "This notebook goes over how to use Redis to store chat message history. It also explains how to write messages to Redis and use it to store them in a database. Redis Chat Message History Zep Memory Agents Callbacks Modules"]}
]